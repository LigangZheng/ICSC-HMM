:::::::::::::::::::::::::::::::::Data Analysis:::::::::::::::::::::::::::::::

--------------------------- Dataset description ---------------------------
The given dataset consists of 5 MoCap files captured via the XSens suite. Each file corresponds to the actions performed by the operator in the CK-12 environment. These actions are:

- RemovingGuard 
- Cleaning (Brushing) 
- RemovingPanel 
- Changing Belt 
- Placing Panel. 

Each MoCap file is composed of a 72 dimensional timeseries corresponding to raw joint readings from 23 joints describing a human skeleton, specifically:


TABLE X: MoCap Joint Data Description
Joint # | Joint Description | Dimensionality
----------------------------------------------
1		| Hips              | [6d] (tx, ty, tz, rx, ry, rz )
2 	    | Chest             | [3d] (rx, ry, rz)
3		| Chest2            | [3d] (rx, ry, rz)
4 		| Chest3            | [3d] (rx, ry, rz)
5 		| Chest4            | [3d] (rx, ry, rz)
6       | Head              | [3d] (rx, ry, rz)
7 		| Neck              | [3d] (rx, ry, rz)
8       | RightCollar       | [3d] (rx, ry, rz)
9       | RightShoulder     | [3d] (rx, ry, rz)
10      | RightElbow        | [3d] (rx, ry, rz)
11      | RightWrist        | [3d] (rx, ry, rz)
12      | LeftCollar        | [3d] (rx, ry, rz)
13      | LeftShoulder      | [3d] (rx, ry, rz)
14      | LeftElbow         | [3d] (rx, ry, rz)
15      | LeftWrist         | [3d] (rx, ry, rz)
16      | RightHip          | [3d] (rx, ry, rz)
17      | RightKnee         | [3d] (rx, ry, rz)
18      | RightAnkle        | [3d] (rx, ry, rz)
19      | RightToe      	| [3d] (rx, ry, rz)
20      | LeftHip       	| [3d] (rx, ry, rz)
21      | LeftKnee          | [3d] (rx, ry, rz)
22      | LeftAnkle         | [3d] (rx, ry, rz)
23      | LeftToe           | [3d] (rx, ry, rz)

Each joint reading can have up to 6 dimensions: tx, ty, tz, rx, ry , rz.

tx, ty, tz correspond to translational position in the global reference frame.
rx, ry, rz are the joint rotation angles corresponding to flexion/extension (x), abductin/adduction (y) and longitudinal rotation (z).

Such data looks like this:
Fig. 1
{Example of Raw data from one action}

As can be seen, using the raw data to try and extract any type of action sequence is infeasible, we should pre-process the data before doing any type of segmentation or recognition.

--------------------------- Dataset Pre-Processing ---------------------------
Many of the tracked joints are redundant and not relevant for the actions at hand. We select a subset of these joints, namely, 12 joints, reducing the dimensionality to 36:



TABLE X: Selected Joint Data for Action Segmentation
Joint # | Joint Description | Dimensionality
----------------------------------------------
1		| Hips              | [3d] (tx, ty, tz)
6       | Head              | [3d] (rx, ry, rz)
9       | RightShoulder     | [3d] (rx, ry, rz)
10      | RightElbow        | [3d] (rx, ry, rz)
11      | RightWrist        | [3d] (rx, ry, rz)
13      | LeftShoulder      | [3d] (rx, ry, rz)
14      | LeftElbow         | [3d] (rx, ry, rz)
15      | LeftWrist         | [3d] (rx, ry, rz)
16      | RightHip          | [3d] (rx, ry, rz)
17      | RightKnee         | [3d] (rx, ry, rz)
20      | LeftHip       	| [3d] (rx, ry, rz)
21      | LefttKnee         | [3d] (rx, ry, rz)


Once we have this subset of dimensions, we can apply some pre-processing. This is necessar for following reasons:
1) Any type of motion capture data is quite noisy (due to measurement errors), hence, we need to smooth out the signals.
2) We can see that for some joint angle signals there is a sudden change in trigonometric quadrants, leading to a sudden discontinuity in the signal. Joint angle discontinuities from the Xsens system are most likely to occur when a subject reverses direction, thus, we need to deal with these discontinuities. 

Fig. 2
{Example of Processed data from one action}

----------------------- Automatic Sub-Action Segmentation -----------------------

The task of discover a set of sub-actions within a complex sequential action is not a straight-fowrard task. Given apriori knowledge on the number of expected sub-actions or some type of representation as a generative or discriminitive model would make this an easily solvable problem. Yet, much of the data that are collected in experiments such as this one is highly unstructured. Hence, any type of recognition algorithm would fail in this situation.

Bayesian nonparametric approaches, on the other hand, allow for learning problems in sequential data to be independent of a priori knowledge of model parameters such as the number of hidden states, cluster or mixtures. For thi reason, we use an algorithm introduced by Fox et al (2009), the Beta Process Hidden Markov Model (BP-HMM), an algorithm capable of modeling multiple sequences with different switching behaviors from an unbounded set of shared behaviors. In our context, these shared behaviors correspond to sub-actions that compose the full action 

We consider a set of N MoCap recordings ξ = {X1 , X2 , . . . , XN }, each one corresponding to a time-series of variable length Xi = {x1 , x2 , . . . , xT }, where xt =
{hips.tz, head.rx,... } represents the observed data from kinesthetic teach-
ing, comprising of 21 joint readings as described in Table Y. 

We then apply the BP-HMM on ξ to learn a set of linear dynamical models Θ = {θ1 , θ2 , . . . θK }, modeled using Gaussian emissions (Figure 1.2). Hence, the model parameters for each primitive θk = {μk , Σk } parametrize the observation dynamics xt ∼ N (μk , Σk ). Moreover, we also learn the latent variables; i.e. states zt which indicate the action primitive at time t and consequently the transition matrix π (i) of the action primitives present in each time series.


Fig. 3
{Schematic of BP-HMM applied to Mocap Data}


When applied to our distint action sequences, we see that we are able to segment blabla


Furthermore, from a total set of Y subactions we recover a set of X shared sub-actions through-out the distinct actions. This will help us create a library of sub-actions that can be re-used for different high-level actions. 

Fig.4 Segmentations

?




